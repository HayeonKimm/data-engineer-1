# 210810

* 데이터수집 파이프라인

  * crontab을 이용하여 주기적으로 실행

    * 1분간격으로 crawl_bus.sh 실행
    * 1시간간격으로 crawl_weather.sh 실행

  * crawl_bus.sh/crawl_weather.sh

    * conda activate 한 뒤 crawl_bus.py 실행 후 conda deactivate
    * conda activate 한 뒤 crawl_weather.py 실행 후 conda deactivate

  * crawl_bus.py/crawl_weather.py

    * crawl_bus.py

      * 양재역 ~ 인제대학교 서울백병원을 공통경로로 가지는 버스 수집

      * 140, 470, 741번 버스

      * 3개의 버스에 대하여 3개의 key를 이용해 수집

        ```python
        buses = [
            '100100019',
            '100100073',
            '123000010',
        ]
        ```

    * crawl_weather.py

      * 양재역 ~ 인제대학교 서울백병원이 위치하는 중구, 용산구, 강남구, 서초구를 포함하는 모든 지역 수집
      * 중구, 용산구, 강남구, 서초구
      * x : 59~62, y : 124~127

  * 최종적으로 다음 장소에 파일이 저장됨

    * /home/lab01/bus
      * 20210810100402Bus123000010.json
    * /home/lab01/weather
      * 20210810095902Weatherx61y124.json

* 만났던 문제들

  * cron을 이용해 conda activate를 바로 실행하면 실행하는 셸이 달라서 정상적으로 실행이 안됨
    * shell file을 작성하고, 그 파일을 bash -i(인터렉티브 모드)로 실행하는 것으로 해결

* mysql과 spark
  * https://towardsdatascience.com/pyspark-mysql-tutorial-fa3f7c26dc7
* 어떻게 파일을 읽는가
  * python을 이용해 pyspark사용(인터넷 검색) <- 채택
  * scala를 이용해 spark-shell사용(책 참고 가능, 명령어 치는데에 어려움 있음)

